{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26704bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/07 21:00:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4b614139906b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe76c2b55e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test to check the version of spark and if it is instanciated\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce658a",
   "metadata": {},
   "source": [
    "### Examples to get you up to speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a7db1",
   "metadata": {},
   "source": [
    "Create a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af371a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''create database if not exists dal.test''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62f14c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|     test|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''show databases''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631a5b74",
   "metadata": {},
   "source": [
    "Create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acd7863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating test table \n",
    "spark.sql('''\n",
    "create table dal.test.test \n",
    "    (\n",
    "    customer_id varchar(64),\n",
    "    company_name varchar(64),\n",
    "    industries varchar(64)\n",
    " ) \n",
    " USING iceberg\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5bbec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+\n",
      "|customer_id|company_name|industries|\n",
      "+-----------+------------+----------+\n",
      "+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select * from dal.test.test''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9364f753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into dal.test.test values('1','2','3')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38f095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+\n",
      "|customer_id|company_name|industries|\n",
      "+-----------+------------+----------+\n",
      "|          1|           2|         3|\n",
      "+-----------+------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from dal.test.test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41aedfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''create database if not exists dal.assignment''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "460c0690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "minio_access_key = \"mini0\"\n",
    "minio_secret_key = \"minio123\"\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", minio_access_key)\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", minio_secret_key)\n",
    "\n",
    "minio_endpoint = \"http://minio:9000\" \n",
    "minio_bucket = \"demo-data\"\n",
    "hist_data = \"historical_orders.json\"\n",
    "recent_data = \"recent_orders.json\"\n",
    "cust_data = \"Customers.csv\"\n",
    "\n",
    "\n",
    "# Read data from MinIO\n",
    "\n",
    "hist_data_path = f\"s3a://{minio_bucket}/{hist_data}\"\n",
    "df_hist = spark.read.format(\"json\").load(hist_data_path)\n",
    "\n",
    "recent_data_path = f\"s3a://{minio_bucket}/{recent_data}\"\n",
    "df_recent = spark.read.format(\"json\").load(recent_data_path)\n",
    "\n",
    "cust_data_path = f\"s3a://{minio_bucket}/{cust_data}\"\n",
    "df_cust = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(cust_data_path)\n",
    "\n",
    "df_hist.write.mode(\"overwrite\").saveAsTable(\"assignment.historical_orders\")\n",
    "df_recent.write.mode(\"overwrite\").saveAsTable(\"assignment.recent_orders\")\n",
    "df_cust.write.mode(\"overwrite\").saveAsTable(\"assignment.customers\")\n",
    "\n",
    "#df_hist.show(5)\n",
    "#df_recent.show(5)\n",
    "#df_cust.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13418f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Explode the order_lines array to get individual rows for each product\n",
    "#order_lines_df = combined_df.withColumn(\"order_line\", explode(col(\"order_lines\"))).drop(\"order_lines\")\n",
    "\n",
    "\n",
    "# Step 6: Explode the specialized_industries array to get individual rows for each industry\n",
    "#customer_industries_df = customer_df.withColumn(\"specialized_industry\", explode(split(col(\"specialized_industries\"), \";\"))).drop(\"specialized_industries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7cc6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, sum, split, expr\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Create a SparkSession\n",
    "#spark = SparkSession.builder.appName(\"IndustryFluctuation\").getOrCreate()\n",
    "\n",
    "historical_df = spark.sql(\"select * from dal.assignment.historical_orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e046a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_df = spark.sql(\"select * from dal.assignment.recent_orders\")\n",
    "\n",
    "# Step 3: Combine historical and recent order data into a single DataFrame\n",
    "combined_orig_df = historical_df.union(recent_df)\n",
    "combined_df = combined_orig_df.dropDuplicates([\"order_id\"])\n",
    "\n",
    "# Step 5: Load customer data from customer.csv and de-duplicate it based on customer_id\n",
    "customer_orig_df = spark.sql(\"select * from dal.assignment.customers\")\n",
    "customer_df = customer_orig_df.dropDuplicates([\"customer_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39329962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-20 23:58:19.767981\n",
      "2023-03-19 23:58:19.767981\n",
      "2023-02-18 23:58:19.767981\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, date_sub\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "combined_date_cast_df = combined_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Find the maximum timestamp date\n",
    "max_date = combined_date_cast_df.select(max(col(\"timestamp\"))).first()[0]\n",
    "one_day_before = max_date - timedelta(days=1)\n",
    "month_before = max_date - timedelta(days=30)\n",
    "\n",
    "print(max_date)\n",
    "print(one_day_before)\n",
    "print(month_before)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec2fa741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Join customer data with order data and de-duplicate it based on order_id\n",
    "order_with_customer_df = combined_date_cast_df.join(customer_df, on=\"customer_id\")\n",
    "\n",
    "# Step 7: Explode the specialized_industries array to get individual rows for each industry\n",
    "customer_industries_df = order_with_customer_df.withColumn(\"specialized_industry\", explode(split(col(\"specialized_industries\"), \";\"))).drop(\"specialized_industries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19735661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+\n",
      "|         customer_id|            amount|            order_id|         order_lines|           timestamp|company_name|specialized_industry|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+\n",
      "|06ec6a8a-c3d9-4b0...|         244413.25|00007ef8-0255-47e...|[{97.71, 4b688bad...| 2022-03-13 19:48:00|     Skilith|             Colours|\n",
      "|54920ba7-09fa-427...|          81861.76|000670bb-8405-456...|[{90.77, 1a4d50ec...|2022-03-07 02:04:...|    Innotype|             Polymer|\n",
      "|0674efa6-ea2c-431...|         126424.82|0007216d-2e34-4df...|[{49.23, 7f96c322...|2022-04-29 21:26:...|     Zoombox|        Construction|\n",
      "|b9b1b6d9-4e0e-457...|29282.679999999997|00097d87-39f7-4a8...|[{6.15, 763bcbb2-...|2022-11-08 06:40:...|     Wordify|           Cosmetics|\n",
      "|b9b1b6d9-4e0e-457...|29282.679999999997|00097d87-39f7-4a8...|[{6.15, 763bcbb2-...|2022-11-08 06:40:...|     Wordify|     Pharmaceuticals|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_industries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aead145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            order_id|    average_amount|\n",
      "+--------------------+------------------+\n",
      "|0152349b-ff99-47e...| 81030.42000000001|\n",
      "|01ca462b-3313-4aa...|          88546.96|\n",
      "|01da1165-ec2d-48e...| 94298.34000000001|\n",
      "|031b4b25-f9de-475...|240275.78999999992|\n",
      "|0370b16a-9dab-4a5...|129972.78000000003|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "\n",
    "# Calculate the average amount per order_id\n",
    "average_amount_by_order = customer_industries_df.groupBy(\"order_id\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
    "\n",
    "# Show the result\n",
    "average_amount_by_order.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75cc1b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Calculate Industry-Wise Daily Revenue for the last 24 hours\n",
    "last_24_hours_df = customer_industries_df.filter(col(\"timestamp\") >= one_day_before)\n",
    "last_30_days_df = customer_industries_df.filter(col(\"timestamp\") >= month_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7ea08ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57261\n",
      "889\n",
      "1446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 105:============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(customer_industries_df.count())\n",
    "print(last_24_hours_df.count())\n",
    "print(last_30_days_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f97458a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_daily_revenue_df = last_24_hours_df.groupBy(\"specialized_industry\").agg(sum(col(\"amount\")).alias(\"daily_revenue\"))\n",
    "industry_monthly_revenue_df = last_30_days_df.groupBy(\"specialized_industry\").agg(avg(col(\"amount\")).alias(\"monthly_avg_revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7373e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|specialized_industry|       daily_revenue|\n",
      "+--------------------+--------------------+\n",
      "|                Food|1.1371096509999996E7|\n",
      "|             Colours|           5773935.9|\n",
      "|             Polymer|1.4475883969999995E7|\n",
      "|             Oil&Gas|   5559699.649999999|\n",
      "|        Construction|1.5049691109999998E7|\n",
      "|         Agriculture|1.1477563790000005E7|\n",
      "|           Cosmetics|1.6000598300000012E7|\n",
      "|            Cleaning|   6498867.699999999|\n",
      "|          Lubricants|   7030005.009999999|\n",
      "|     Pharmaceuticals| 1.543688640000001E7|\n",
      "|             Ploymer|  2253577.7300000004|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|specialized_industry|     monthly_revenue|\n",
      "+--------------------+--------------------+\n",
      "|                Food|       1.944055448E7|\n",
      "|             Colours|1.1263576360000003E7|\n",
      "|             Polymer| 2.347341588000001E7|\n",
      "|             Oil&Gas|       1.072537522E7|\n",
      "|        Construction|2.6396147090000004E7|\n",
      "|         Agriculture|1.7755922129999995E7|\n",
      "|           Cosmetics| 2.544358884000002E7|\n",
      "|            Cleaning|1.2208112370000005E7|\n",
      "|          Lubricants|1.1242831620000001E7|\n",
      "|     Pharmaceuticals|2.3898277740000017E7|\n",
      "|             Ploymer|   4472038.580000002|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "industry_daily_revenue_df.show()\n",
    "industry_monthly_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6a931d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|specialized_industry|monthly_avg_revenue|\n",
      "+--------------------+-------------------+\n",
      "|                Food| 131355.09783783785|\n",
      "|             Colours| 122430.17782608699|\n",
      "|             Polymer| 124197.96761904767|\n",
      "|             Oil&Gas| 130797.25878048781|\n",
      "|        Construction| 126904.55331730771|\n",
      "|         Agriculture| 131525.34911111108|\n",
      "|           Cosmetics|  129814.2287755103|\n",
      "|            Cleaning| 129873.53585106388|\n",
      "|          Lubricants| 135455.80265060242|\n",
      "|     Pharmaceuticals| 129179.87967567577|\n",
      "|             Ploymer|  131530.5464705883|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "industry_monthly_revenue_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6fee2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 146:============================>                            (1 + 1) / 2]\r",
      "\r",
      "[Stage 146:========>        (1 + 1) / 2][Stage 147:========>        (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|specialized_industry|       daily_revenue|monthly_avg_revenue|         fluctuation|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "|           Cosmetics|1.6000598300000012E7|  129814.2287755103|1.5870784071224501E7|\n",
      "|     Pharmaceuticals| 1.543688640000001E7| 129179.87967567577|1.5307706520324335E7|\n",
      "|        Construction|1.5049691109999998E7| 126904.55331730771| 1.492278655668269E7|\n",
      "+--------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "industry_fluctuation_df = industry_daily_revenue_df.join(industry_monthly_revenue_df, on=\"specialized_industry\")\n",
    "industry_fluctuation_cal_df = industry_fluctuation_df.withColumn(\"fluctuation\", col(\"daily_revenue\") - col(\"monthly_avg_revenue\"))\n",
    "\n",
    "# Step 11: Identify Top 3 Industries with the Highest Fluctuations (Positive or Negative)\n",
    "top_3_industries_df = industry_fluctuation_cal_df.orderBy(col(\"fluctuation\").desc()).limit(3)\n",
    "\n",
    "# Step 12: Show the results\n",
    "top_3_industries_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228a88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e52380f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 30 days, 0:00:00 of type <class 'datetime.timedelta'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m\n\u001b[1;32m     19\u001b[0m calculate_boundary_udf \u001b[38;5;241m=\u001b[39m udf(calculate_boundary, LongType())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 9: Calculate Industry-Wise Average Revenue for the past 30 days\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\").rangeBetween(expr(\"-interval 30 days\"), expr(\"-interval 1 day\"))\\\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#                    .rangeBetween(calculate_boundary_udf(interval_30_days), calculate_boundary_udf(interval_1_day))\\\u001b[39;00m\n\u001b[1;32m     32\u001b[0m window_spec \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecialized_industry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m---> 33\u001b[0m                     \u001b[38;5;241m.\u001b[39mrangeBetween(\u001b[43mcalculate_boundary_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_30_days\u001b[49m\u001b[43m)\u001b[49m, calculate_boundary_udf(interval_1_day))\n\u001b[1;32m     35\u001b[0m industry_avg_revenue_df \u001b[38;5;241m=\u001b[39m order_with_customer_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_revenue_last_30_days\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_line.volume\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_line.price\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mover(window_spec))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Step 10: Calculate Fluctuation for each Industry\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#industry_fluctuation_df = industry_daily_revenue_df.join(industry_avg_revenue_df, on=\"specialized_industry\")\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#industry_fluctuation_df = industry_fluctuation_df.withColumn(\"fluctuation\", col(\"daily_revenue\") - col(\"avg_revenue_last_30_days\"))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Step 13: Stop the SparkSession\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#spark.stop()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/udf.py:276\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/udf.py:251\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf\n\u001b[0;32m--> 251\u001b[0m jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m jPythonUDF\u001b[38;5;241m.\u001b[39mexpr()\u001b[38;5;241m.\u001b[39mresultId()\u001b[38;5;241m.\u001b[39mid()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 30 days, 0:00:00 of type <class 'datetime.timedelta'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "end_date = datetime.now()\n",
    "#start_date_30_days = end_date - timedelta(days=30)\n",
    "#start_date_1_day = end_date - timedelta(days=1)\n",
    "\n",
    "# Define the time intervals\n",
    "interval_30_days = timedelta(days=30)\n",
    "interval_1_day = timedelta(days=1)\n",
    "\n",
    "def calculate_boundary(interval):\n",
    "    return -interval.total_seconds()\n",
    "\n",
    "# Register the UDF\n",
    "calculate_boundary_udf = udf(calculate_boundary, LongType())\n",
    "\n",
    "\n",
    "\n",
    "# Step 9: Calculate Industry-Wise Average Revenue for the past 30 days\n",
    "#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\").rangeBetween(expr(\"-interval 30 days\"), expr(\"-interval 1 day\"))\\\n",
    "#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\n",
    "#                    .rangeBetween(-int((end_date - start_date_30_days).total_seconds()), -1)\n",
    "#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\n",
    "#                    .rangeBetween(-interval_30_days, -interval_1_day)\n",
    "#window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\n",
    "#                    .rangeBetween(calculate_boundary_udf(interval_30_days), calculate_boundary_udf(interval_1_day))\\\n",
    "\n",
    "window_spec = Window.partitionBy(\"specialized_industry\").orderBy(\"timestamp\") \\\n",
    "                    .rangeBetween(calculate_boundary_udf(interval_30_days), calculate_boundary_udf(interval_1_day))\n",
    "\n",
    "industry_avg_revenue_df = order_with_customer_df.withColumn(\"avg_revenue_last_30_days\", sum(col(\"order_line.volume\") * col(\"order_line.price\")).over(window_spec))\n",
    "\n",
    "# Step 10: Calculate Fluctuation for each Industry\n",
    "#industry_fluctuation_df = industry_daily_revenue_df.join(industry_avg_revenue_df, on=\"specialized_industry\")\n",
    "#industry_fluctuation_df = industry_fluctuation_df.withColumn(\"fluctuation\", col(\"daily_revenue\") - col(\"avg_revenue_last_30_days\"))\n",
    "\n",
    "# Step 11: Identify Top 3 Industries with the Highest Fluctuations (Positive or Negative)\n",
    "#top_3_industries_df = industry_fluctuation_df.orderBy(col(\"fluctuation\").desc()).limit(3)\n",
    "\n",
    "# Step 12: Show the results\n",
    "#top_3_industries_df.show()\n",
    "\n",
    "# Step 13: Stop the SparkSession\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ba6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
